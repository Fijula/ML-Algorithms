What It Is:
A supervised learning algorithm used for both classification and regression.
It models decisions using a tree-like structure.
The algorithm splits data into branches (nodes) by asking yes/no or true/false questions based on features.
You start at the root, answer questions at each node, and move down to a leaf node, which gives the final prediction.
💡 Real-Life Analogy:
Do I need an umbrella?
A decision tree might work like this:

Is it cloudy?
   └── No → Don’t take umbrella
   └── Yes → Will it rain?
             └── Yes → Take umbrella
             └── No  → Don’t take umbrella
This is how Decision Trees work — by asking questions and making decisions step-by-step.

📘 Toy Dataset Example:
Cloudy (1/0)	Rain (1/0)	Take Umbrella (yes/no)
1	1	yes
1	0	no
0	1	yes
0	0	no
✅ Python Code Example:
from sklearn.tree import DecisionTreeClassifier

# X: [Cloudy, Rain]
X = [[1, 1], [1, 0], [0, 1], [0, 0]]

# y: Take umbrella? yes/no
y = ['yes', 'no', 'yes', 'no']

# Train decision tree
model = DecisionTreeClassifier()
model.fit(X, y)

# Predict for cloudy and raining
prediction = model.predict([[1, 1]])
print(f"Prediction: {prediction[0]}")  # Output: yes
🧾 Output:
Prediction: yes
📈 Diagram (Text-Based Tree):
Let’s draw a simple tree based on the dataset:

           [Cloudy?]
           /      \
         Yes      No
        /           \
   [Rain?]         [Rain?]
    /   \           /    \
  Yes   No       Yes     No
  |      |        |       |
yes     no      yes      no
You follow the tree based on conditions (features), and each path ends in a final decision.

🎯 When to Use Decision Trees:
Situation	Usefulness
If-else decision-making logic	✅ Perfect
You want easy-to-understand output	✅ Yes
Small to medium-sized datasets	✅ Good
Large or noisy data	⚠️ Risk of overfitting
📦 Pros and Cons:
✅ Pros	❌ Cons
Easy to understand and visualize	Can easily overfit
No need to scale features	Sensitive to slight data changes
Handles both numerical and categorical data	Trees can become very deep and complex
Can show feature importance	Less accurate than ensemble methods
