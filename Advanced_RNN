RNN (Recurrent Neural Network) / LSTM (Long Short-Term Memory)

ğŸ” What it is:
A neural network where output from previous steps feeds into the current step, giving it memory.

Useful for sequential data like text or time series.
LSTM is a type of RNN that solves the vanishing gradient problem, so it remembers information over long sequences.
ğŸ§  Diagram:
Input_t1 â†’ [Memory_1] â†’ Input_t2 â†’ [Memory_2] â†’ ... â†’ Output
ğŸŒ Real-life Examples:
Text prediction (e.g., auto-complete)
Language translation (e.g., Google Translate)
Stock forecasting
Speech-to-text
ğŸ‘¨â€ğŸ’» Code:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import numpy as np

# Dummy time-series data
X = np.random.rand(100, 10, 1)  # (samples, time_steps, features)
y = np.random.rand(100, 1)

model = Sequential([
    LSTM(64, input_shape=(10, 1)),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=3)
âœ… When to Use:
Time series forecasting
Text generation or classification
Sequence prediction
